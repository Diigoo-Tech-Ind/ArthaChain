# Decentralized AI Training Architecture on ArthaChain
## Complete Design Document for 1000 Laptop Distributed Training

---

## üî• SECTION 1 ‚Äî On-Chain Layer (Smart Contracts)

### 1. Smart Contract Design for AI Compute Node Management

Based on `NodeCertRegistry.sol` and `AIJobManager.sol`, here's the complete design:

#### **Node Registration**

**Contract:** `NodeCertRegistry.sol`

```solidity
function registerNode(
    bytes32 nodePubkey,
    NodeRole role,        // GPUProvider for AI compute
    string calldata region,
    bytes32 caps,         // Encoded capabilities
    bytes32 slaId
) external payable returns (bytes32)
```

**Registration Requirements:**
- **Minimum Stake:** 1 ARTH (configurable via `minStake`)
- **Node Public Key:** `bytes32` - Cryptographic identity
- **Capabilities Encoding:**
  - `hasGPU: bool` (bit 0)
  - `hasTEE: bool` (bit 1)
  - `diskGB: uint32` (bits 8-39)
  - `bandwidthMbps: uint32` (bits 40-71)
  - `computeUnits: uint32` (bits 72-103)

**On-Chain Storage:**
```solidity
struct NodeCert {
    bytes32 nodePubkey;
    address operator;
    NodeRole role;
    string region;
    bytes32 caps;
    bytes32 slaId;
    uint256 stake;
    uint64 registeredAt;
    uint64 lastHeartbeat;
    bool active;
}
```

#### **Hardware Verification**

**Off-Chain Verification Process:**
1. **GPU Detection:** Node runs `nvidia-smi` or equivalent
2. **Hardware Attestation:** 
   - CPU model, cores, RAM
   - GPU model, VRAM, compute capability
   - Disk space, bandwidth test
3. **TEE Attestation (Optional):** Intel SGX DCAP quotes for secure compute
4. **On-Chain Commit:** Hash of hardware spec committed to `caps` field

**Verification Smart Contract:**
```solidity
// In NodeCertRegistry
function verifyHardware(bytes32 nodePubkey, bytes32 hwHash, bytes calldata attestation) external {
    // Verify attestation signature
    // Update node capabilities
    // Emit HardwareVerified event
}
```

#### **Staking Requirements**

**Tiered Staking Model:**
- **Basic Node:** 1 ARTH minimum
- **GPU Node:** 10 ARTH (for GPUProvider role)
- **TEE Node:** 50 ARTH (for secure compute)
- **High-Reputation Node:** 100+ ARTH (for priority job assignment)

**Stake Slashing Conditions:**
- Missed heartbeat > 1 hour: Warning
- 3 consecutive missed heartbeats: 10% stake slash
- Failed job completion: Proportional to job budget
- Malicious gradient submission: 50% stake slash
- Cheating detection: 100% stake slash + deactivation

#### **Reputation Score**

**Contract:** `ReputationRegistry.sol`

**Reputation Metrics (7 dimensions):**
1. **Total Deals / Successful Deals** (uptime score 0-10000 bp)
2. **Violation Count / Slash Count** (penalty score)
3. **Proof Success Rate** (0-10000 bp)
4. **Bandwidth Score** (0-10000 bp)
5. **Lineage Contributions** (model/dataset contributions)
6. **Slash-Free Epochs** (consecutive good behavior)
7. **Job Completion Rate** (successful vs failed jobs)

**Reputation Calculation:**
```solidity
reputation = (
    (uptime_score * 0.3) +
    (proof_success_rate * 0.25) +
    (bandwidth_score * 0.15) +
    (lineage_contribs * 0.1) +
    (slash_free_epochs * 0.1) +
    (job_completion_rate * 0.1)
) * (1 - violation_penalty)
```

**Max Reputation:** 1,000,000 (capped)

---

### 2. Committing Training Results to Blockchain

**Contract:** `ProofOfCompute.sol`

#### **Hash Commits Strategy**

**Training Proof Structure:**
```solidity
struct TrainProof {
    bytes32 jobId;
    uint256 step;
    bytes32 lossDigest;      // Keccak256(loss_values)
    bytes32 gradientDigest;  // Keccak256(serialized_gradients)
    bytes32 weightsDigest;  // Keccak256(updated_weights)
    uint64 timestamp;
    bytes32 nodePubkey;
    bytes signature;         // Ed25519 signature
}
```

**Commitment Flow:**
1. **Per-Step Commitments:**
   - Every N steps (e.g., every 100 steps), node computes:
     - `lossDigest = keccak256(loss_values)`
     - `gradientDigest = keccak256(flattened_gradients)`
     - `weightsDigest = keccak256(serialized_weights)`
   - Signs with node private key
   - Calls `recordTrainProof()` on-chain

2. **Gradient Summaries:**
   - Instead of full gradients, commit:
     - Gradient statistics (mean, std, min, max per layer)
     - Gradient norm (L2 norm)
     - Top-K gradient values (for verification)

3. **Model Deltas:**
   - Commit weight deltas: `ŒîW = W_new - W_old`
   - Hash of delta tensor per layer
   - Merkle root of all layer deltas

**On-Chain Storage:**
```solidity
mapping(bytes32 => TrainProof[]) public trainProofs;  // jobId -> proofs
```

**Gas Optimization:**
- Batch commits: Submit multiple steps in one transaction
- Use blob storage (EIP-4844) for large gradient summaries
- Off-chain storage: Full gradients stored in SVDB, only hash on-chain

---

### 3. Verification: Ensuring Nodes Actually Did Training

#### **Option 1: Redundancy (Primary Method)**

**Multi-Node Verification:**
- Assign same training task to 3-5 nodes
- Compare gradient digests
- If >50% match ‚Üí valid
- If mismatch ‚Üí investigate outliers

**Implementation:**
```solidity
function verifyByRedundancy(bytes32 jobId, bytes32[] calldata nodePubkeys) external view returns (bool) {
    TrainProof[] memory proofs = trainProofs[jobId];
    // Count matching gradient digests
    // Require >50% consensus
}
```

#### **Option 2: ZK Proofs for ML (zkML)**

**zkML Circuit Design:**
- Prove: "I computed gradients correctly from input data"
- Use Groth16 SNARK (BN254 curve)
- Circuit proves:
  - Forward pass computation
  - Loss calculation
  - Backward pass (gradient computation)
  - Weight update

**Implementation:**
```solidity
struct ZKMLProof {
    bytes32 publicInput;  // Input data hash
    bytes32 outputHash;    // Output hash
    bytes proof;          // Groth16 proof
}

function verifyZKMLProof(ZKMLProof calldata proof) external view returns (bool) {
    // Verify SNARK proof on-chain
    // Check public input matches committed input
}
```

**Trade-offs:**
- ‚úÖ Cryptographically secure
- ‚ùå High proving cost (GPU required, ~8.5s per proof)
- ‚ùå Circuit complexity for large models

#### **Option 3: Random Challenge Tests**

**Challenge-Response Protocol:**
1. **Challenge Issuance:** Randomly select step N
2. **Node Response:** Must provide:
   - Intermediate activations at step N
   - Gradient values for specific layer
   - Merkle proof of computation path
3. **Verification:** Validator recomputes and verifies

**Implementation:**
```solidity
function issueChallenge(bytes32 jobId, uint256 step, uint256 layer) external {
    // Randomly challenge a specific step/layer
    // Node must respond within time window
}

function respondToChallenge(
    bytes32 jobId,
    bytes32 activationHash,
    bytes32 gradientHash,
    bytes calldata merkleProof
) external {
    // Verify merkle proof
    // Check activation hash matches expected
}
```

#### **Option 4: Sampling Gradients**

**Gradient Sampling Protocol:**
- Randomly sample 1-5% of gradients per layer
- Node commits full gradient hash
- Validator requests sampled gradients
- Node provides sampled values + Merkle proof
- Validator verifies sampled values match hash

**Implementation:**
```solidity
function requestGradientSample(
    bytes32 jobId,
    uint256 step,
    uint256 layer,
    uint256[] calldata indices
) external view returns (uint256[] memory) {
    // Node returns sampled gradient values
    // Validator verifies against committed hash
}
```

**Recommended Hybrid Approach:**
1. **Primary:** Redundancy (3-5 nodes per task)
2. **Secondary:** Random challenge tests (10% of jobs)
3. **Tertiary:** Gradient sampling (spot checks)
4. **Advanced:** zkML for high-value jobs (optional)

---

### 4. Incentive Model for Rewarding Compute Nodes

**Contract:** `ProofOfCompute.sol` + `AIJobManager.sol`

#### **Reward Per Batch**

**Structure:**
```solidity
struct BatchReward {
    bytes32 jobId;
    uint256 batchIndex;
    uint256 gpuSeconds;      // Compute time
    uint256 reward;          // ARTH tokens
}
```

**Calculation:**
- Base rate: 0.001 ARTH per GPU-second
- Batch size multiplier: Larger batches = higher efficiency bonus
- Formula: `reward = gpuSeconds * baseRate * batchMultiplier`

#### **Reward Per Gradient Update**

**Per-Step Rewards:**
- Each gradient update committed = micro-reward
- Accumulated and paid at job completion
- Formula: `stepReward = (jobBudget / totalSteps) * qualityMultiplier`

#### **Reward Per Successful Round**

**Federated Learning Rounds:**
- Each round completion = fixed reward
- Distributed among participating nodes
- Formula: `roundReward = (roundBudget / numParticipants) * contributionScore`

#### **Reward Per Accuracy Improvement**

**Performance-Based Rewards:**
- Track model accuracy improvement
- Bonus for exceeding target accuracy
- Formula: `accuracyBonus = baseReward * (actualAccuracy / targetAccuracy)`

**Final Reward Structure:**
```solidity
function calculateReward(bytes32 jobId) external view returns (uint256) {
    ComputeReceipt memory receipt = receipts[jobId];
    
    uint256 baseReward = receipt.gpuSeconds * 1e15; // 0.001 ARTH/GPU-sec
    
    // Accuracy bonus (if applicable)
    uint256 accuracyBonus = 0;
    if (accuracyImprovement > targetImprovement) {
        accuracyBonus = baseReward * (accuracyImprovement / targetImprovement) / 100;
    }
    
    // Quality multiplier (based on proof verification)
    uint256 qualityMultiplier = getQualityMultiplier(jobId);
    
    return (baseReward + accuracyBonus) * qualityMultiplier / 100;
}
```

**Recommended Model:**
- **Primary:** Per successful round (federated learning)
- **Secondary:** Per batch (for single-node training)
- **Bonus:** Accuracy improvement (performance incentive)
- **Penalty:** Slashing for failed/cheating nodes

---

## üî• SECTION 2 ‚Äî Off-Chain Compute Layer (Real Training)

### 5. Parts of AI Training Pipeline That Must Run Off-Chain

**Everything Heavy Runs Off-Chain:**

1. **Forward Pass:**
   - Input data loading
   - Layer-by-layer computation
   - Activation storage
   - Output generation

2. **Backward Pass:**
   - Loss computation
   - Gradient computation (via backpropagation)
   - Gradient accumulation
   - Gradient clipping/normalization

3. **Optimization:**
   - Optimizer step (Adam, SGD, etc.)
   - Weight updates
   - Learning rate scheduling
   - Momentum updates

4. **Data Processing:**
   - Data loading and batching
   - Data augmentation
   - Preprocessing

**On-Chain (Minimal):**
- Job assignment
- Proof commitments (hashes only)
- Reward distribution
- Reputation updates

**Storage Strategy:**
- **Model Weights:** SVDB (IPFS-like storage)
- **Training Data:** SVDB with encryption
- **Gradients:** SVDB (full), blockchain (hash only)
- **Checkpoints:** SVDB, CID stored on-chain

---

### 6. Splitting Model Across 1000 Laptops

#### **Option 1: Data Parallelism (Recommended for 1000 Laptops)**

**How It Works:**
- Each laptop gets a copy of the full model
- Each laptop processes different data batches
- Gradients aggregated via Federated Averaging (FedAvg)

**Implementation:**
```rust
// From services/ai-federation/src/main.rs
fn federated_average(updates: &[GradientUpdate]) -> Vec<f64> {
    let total_samples: u64 = updates.iter().map(|u| u.sample_count).sum();
    let num_params = updates[0].weights.len();
    let mut aggregated = vec![0.0; num_params];
    
    // Weighted average: sum(w_i * n_i) / sum(n_i)
    for update in updates {
        let weight = update.sample_count as f64 / total_samples as f64;
        for (i, w) in update.weights.iter().enumerate() {
            aggregated[i] += w * weight;
        }
    }
    
    aggregated
}
```

**Advantages:**
- ‚úÖ Simple to implement
- ‚úÖ Works with heterogeneous hardware
- ‚úÖ Fault tolerant (nodes can drop)
- ‚úÖ No inter-node communication during training

**Disadvantages:**
- ‚ùå Requires full model to fit in each laptop's VRAM
- ‚ùå Slower convergence (communication overhead)

**Bandwidth Requirements:**
- Per round: Model size √ó 2 (upload gradients + download aggregated)
- For 1B parameter model (4GB): ~8GB per round
- Round frequency: Every 10-100 local steps

#### **Option 2: Model Parallelism**

**How It Works:**
- Split model layers across nodes
- Each node processes same batch through its layers
- Activations passed between nodes

**Layer Distribution:**
```
Node 1: Layers 0-10
Node 2: Layers 11-20
Node 3: Layers 21-30
...
Node 100: Layers 990-1000
```

**Advantages:**
- ‚úÖ Can train models larger than single node VRAM
- ‚úÖ Efficient for very large models

**Disadvantages:**
- ‚ùå High inter-node communication (every forward/backward pass)
- ‚ùå Synchronization overhead
- ‚ùå One slow node blocks entire pipeline
- ‚ùå Complex fault tolerance

**Not Recommended for 1000 Laptops:** Too much communication overhead

#### **Option 3: Federated Training (Best for Laptops)**

**How It Works:**
- Each laptop trains on local data
- Periodic aggregation of gradients
- Differential privacy for data protection

**Implementation:**
```rust
// Secure aggregation with differential privacy
fn secure_aggregate(updates: &[GradientUpdate], dp_scale: f64) -> Vec<f64> {
    let mut aggregated = federated_average(updates);
    
    // Add Laplacian noise for differential privacy
    use rand::Rng;
    let mut rng = rand::thread_rng();
    for weight in &mut aggregated {
        let noise = rng.gen::<f64>() * dp_scale;
        *weight += noise;
    }
    
    aggregated
}
```

**Advantages:**
- ‚úÖ Privacy-preserving (data stays on device)
- ‚úÖ Works with heterogeneous data distributions
- ‚úÖ Fault tolerant
- ‚úÖ Low communication (only gradients)

**Disadvantages:**
- ‚ùå Slower convergence
- ‚ùå Requires careful aggregation

#### **Option 4: Hybrid Approach (Recommended)**

**Strategy:**
1. **Intra-Node:** Data parallelism within node clusters (10-20 laptops per cluster)
2. **Inter-Cluster:** Federated learning between clusters
3. **Aggregation:** Hierarchical FedAvg

**Architecture:**
```
Cluster 1 (20 laptops) ‚Üí Local Aggregation ‚Üí Cluster Gradient
Cluster 2 (20 laptops) ‚Üí Local Aggregation ‚Üí Cluster Gradient
...
Cluster 50 (20 laptops) ‚Üí Local Aggregation ‚Üí Cluster Gradient
                    ‚Üì
            Global Aggregation
                    ‚Üì
            Updated Global Model
```

**Benefits:**
- ‚úÖ Reduces communication (20√ó less)
- ‚úÖ Faster local convergence
- ‚úÖ Fault tolerant at cluster level
- ‚úÖ Scalable to 1000+ nodes

**Recommended:** Hybrid approach with federated learning as primary

---

### 7. Maximum Model Size for Distributed Laptops

**Constraints:**
- **VRAM per laptop:** 4-16GB (typical)
- **Bandwidth per laptop:** 10-100 Mbps (typical)
- **Network latency:** 50-200ms (India)

#### **500M Parameters (Recommended)**

**Model Size:**
- Parameters: 500M
- FP16 weights: 1GB
- FP32 weights: 2GB
- With activations: ~4-6GB VRAM

**Feasibility:**
- ‚úÖ Fits in 8GB+ VRAM laptops
- ‚úÖ Can use data parallelism
- ‚úÖ Good for inference (chat, text generation)

**Use Cases:**
- Text generation (GPT-2 scale)
- Classification tasks
- Lightweight chat models

#### **1B Parameters**

**Model Size:**
- Parameters: 1B
- FP16 weights: 2GB
- FP32 weights: 4GB
- With activations: ~8-12GB VRAM

**Feasibility:**
- ‚ö†Ô∏è Requires 12GB+ VRAM laptops
- ‚ö†Ô∏è May need gradient checkpointing
- ‚úÖ Still feasible with data parallelism

**Use Cases:**
- Better chat models
- Multilingual models
- Medium-scale tasks

#### **3B Parameters**

**Model Size:**
- Parameters: 3B
- FP16 weights: 6GB
- FP32 weights: 12GB
- With activations: ~16-24GB VRAM

**Feasibility:**
- ‚ùå Most laptops can't fit (need 16GB+ VRAM)
- ‚ö†Ô∏è Requires model parallelism or gradient checkpointing
- ‚ö†Ô∏è High communication overhead

**Use Cases:**
- High-quality chat models
- Multimodal lite models
- Advanced tasks

#### **Realistic Recommendation:**

**For 1000 Laptops in India:**
- **Target Model:** 500M-1.3B parameters
- **Reasoning:**
  - Most laptops have 8-16GB RAM (not all have dedicated GPU)
  - CPU training possible but slower
  - 1.3B is sweet spot for quality vs. feasibility

**Model Architecture Recommendations:**
- **SSM Models (Mamba/RWKV):** CPU-friendly, efficient
- **Small Transformer:** GPT-2 style, 500M-1.3B
- **Hybrid:** SSM + Transformer layers

---

## üî• SECTION 3 ‚Äî Synchronization & Networking

### 8. Synchronizing Gradient Updates from All Nodes

#### **Technique: Federated Averaging (FedAvg) - Primary**

**Implementation:** (from `services/ai-federation/src/main.rs`)

```rust
fn federated_average(updates: &[GradientUpdate]) -> Vec<f64> {
    let total_samples: u64 = updates.iter().map(|u| u.sample_count).sum();
    let num_params = updates[0].weights.len();
    let mut aggregated = vec![0.0; num_params];
    
    // Weighted average: sum(w_i * n_i) / sum(n_i)
    for update in updates {
        let weight = update.sample_count as f64 / total_samples as f64;
        for (i, w) in update.weights.iter().enumerate() {
            aggregated[i] += w * weight;
        }
    }
    
    aggregated
}
```

**Workflow:**
1. **Collection Phase:** Aggregator collects gradients from all nodes
2. **Weighted Average:** Compute weighted average based on sample counts
3. **Distribution:** Broadcast aggregated model to all nodes
4. **Next Round:** Nodes continue training from updated model

#### **Technique: Partial Updates**

**Strategy:**
- Only aggregate gradients from nodes that completed round
- Use threshold: Wait for 80% of nodes
- Continue with partial updates if nodes drop

**Implementation:**
```rust
async fn trigger_aggregation(fed_id: String) -> Result<Vec<f64>> {
    let updates = gradient_updates.read().await;
    let job = fed_jobs.read().await.get(&fed_id)?;
    
    // Wait for 80% of participants
    let min_updates = (job.participants.len() * 8) / 10;
    if updates.len() >= min_updates {
        return Ok(federated_average(&updates));
    }
    
    Err("Not enough updates")
}
```

#### **Technique: Layer-Wise Merging**

**Strategy:**
- Aggregate gradients layer by layer
- Allows partial updates (some layers updated, others not)
- Useful for very large models

#### **Technique: Server Aggregation**

**Architecture:**
- Central aggregator node (or committee of nodes)
- Nodes send gradients to aggregator
- Aggregator computes FedAvg
- Aggregator broadcasts updated model

**Aggregator Selection:**
- High-reputation nodes
- High-bandwidth nodes
- Geographically distributed (low latency)

**Recommended:** **Federated Averaging with Partial Updates**

---

### 9. Bandwidth Requirements for Nodes

#### **Per-Node Bandwidth Calculation**

**For 1B Parameter Model (FP16):**
- Model size: 2GB
- Gradient size: 2GB (same as model)
- Per round: Upload 2GB + Download 2GB = 4GB

**Round Frequency:**
- Every 10 local steps (typical)
- 1 round per hour (conservative)
- 1 round per 10 minutes (aggressive)

**Bandwidth Requirements:**

| Round Frequency | Upload | Download | Total/Month |
|----------------|--------|----------|-------------|
| 1 round/hour | 2GB | 2GB | ~2.9TB |
| 1 round/10min | 2GB | 2GB | ~17.5TB |
| 1 round/day | 2GB | 2GB | ~120GB |

**Minimum Requirements:**
- **Upload:** 10 Mbps (for 1 round/hour)
- **Download:** 10 Mbps (for 1 round/hour)
- **Total:** 20 Mbps per node

**Realistic for India:**
- **Urban:** 50-100 Mbps (feasible)
- **Rural:** 10-20 Mbps (minimum)
- **Mobile:** 5-10 Mbps (not recommended for training)

**Optimization Strategies:**
1. **Gradient Compression:**
   - Top-K sparsification (keep only top 1% gradients)
   - Quantization (8-bit instead of 16-bit)
   - Reduces gradient size by 10-50√ó

2. **Delta Compression:**
   - Send only gradient deltas (changes)
   - Further reduces size

3. **Asynchronous Updates:**
   - Don't wait for all nodes
   - Update as gradients arrive

**Recommended Minimum:** 20 Mbps per node (for 1B model, 1 round/hour)

---

### 10. Handling Nodes Dropping Offline Mid-Training

#### **Strategy 1: Redundancy (Primary)**

**Implementation:**
- Assign same training task to 3-5 nodes
- If one drops, others continue
- Aggregate from remaining nodes

**Fault Tolerance:**
```rust
async fn handle_node_dropout(job_id: String, dropped_node: String) {
    // Check if enough nodes remain (>=50%)
    let remaining_nodes = get_active_nodes(job_id);
    if remaining_nodes.len() >= min_required_nodes {
        // Continue with remaining nodes
        continue_training(job_id, remaining_nodes);
    } else {
        // Reassign to new nodes
        reassign_job(job_id);
    }
}
```

#### **Strategy 2: Re-Assignment**

**Job Re-Assignment:**
- Detect node dropout (missed heartbeat)
- Reassign job to new node
- New node loads latest checkpoint
- Continues from checkpoint

**Implementation:**
```solidity
// In AIJobManager.sol
function reassignJob(bytes32 jobId, bytes32 newNodePubkey) external {
    Job storage job = jobs[jobId];
    require(job.status == JobStatus.Running, "Job not running");
    
    // Mark old node as failed
    job.assignedNode = newNodePubkey;
    job.status = JobStatus.Assigned;
    
    emit JobReassigned(jobId, newNodePubkey);
}
```

#### **Strategy 3: Fault Tolerance**

**Checkpoint-Based Recovery:**
- Save checkpoints every N steps
- Store checkpoints in SVDB
- On node dropout, load latest checkpoint on new node

**Gradient Buffering:**
- Buffer gradients from nodes
- If node drops, use last received gradient
- Continue aggregation with available gradients

**Recommended:** **Redundancy + Checkpoint-Based Recovery**

---

## üî• SECTION 4 ‚Äî Model Architecture Strategy

### 11. CPU-Friendly vs GPU-Friendly Architecture

#### **CPU-Friendly: SSM Models (Recommended)**

**Mamba/RWKV Architecture:**
- State Space Models (SSM)
- Linear complexity (O(n) vs O(n¬≤) for transformers)
- Efficient on CPU
- Lower memory footprint

**Advantages:**
- ‚úÖ Works on laptops without GPU
- ‚úÖ Lower power consumption
- ‚úÖ Faster inference
- ‚úÖ Good for India (many laptops lack GPU)

**Disadvantages:**
- ‚ùå Newer architecture (less pre-trained models)
- ‚ùå May need custom training code

#### **GPU-Friendly: Transformer Models**

**Standard Transformers:**
- GPT-2, BERT style
- Quadratic complexity
- Requires GPU for efficient training
- Higher memory footprint

**Advantages:**
- ‚úÖ Mature architecture
- ‚úÖ Many pre-trained models
- ‚úÖ Well-understood

**Disadvantages:**
- ‚ùå Requires GPU (not all laptops have)
- ‚ùå Higher power consumption
- ‚ùå Slower on CPU

**Recommendation:** **CPU-Friendly SSM Models (Mamba/RWKV)**
- Better for 1000 laptops (many won't have GPU)
- More inclusive
- Lower barrier to entry

---

### 12. Smallest Practical Model for India

#### **500M Parameters (Inference)**

**Use Cases:**
- Text generation
- Classification
- Lightweight chat

**Quality:**
- Basic conversational ability
- Good for specific domains
- Fast inference

#### **1.3B Parameters (Good Chat) - Recommended**

**Use Cases:**
- High-quality chat
- Multilingual support
- General-purpose tasks

**Quality:**
- Good conversational ability
- Better reasoning
- More versatile

**Feasibility:**
- ‚úÖ Fits in 8-16GB RAM
- ‚úÖ Can train on CPU (slower) or GPU
- ‚úÖ Good balance

#### **3B Parameters (Multimodal Lite)**

**Use Cases:**
- Multimodal (text + images)
- Advanced reasoning
- High-quality generation

**Quality:**
- Excellent conversational ability
- Better reasoning
- More capabilities

**Feasibility:**
- ‚ö†Ô∏è Requires 16GB+ RAM
- ‚ö†Ô∏è May need GPU
- ‚ö†Ô∏è Higher communication overhead

**Recommendation:** **1.3B Parameters**
- Best balance of quality and feasibility
- Works on most laptops
- Good for India's use cases

---

### 13. Model Weight Storage: On-Chain vs IPFS/Filecoin

#### **On-Chain Storage: Impossible**

**Why:**
- 1B parameter model = 2-4GB
- Blockchain block size: ~1-2MB
- Gas cost: Prohibitively expensive
- Not scalable

#### **Off-Chain Storage: SVDB (Recommended)**

**SVDB (Sovereign Verifiable Database):**
- Content-addressed storage (CID-based)
- Similar to IPFS but with verifiable proofs
- On-chain: Only CID (32 bytes)
- Off-chain: Full model weights

**Implementation:**
```solidity
// ModelRegistry.sol
struct Model {
    bytes32 modelCid;  // SVDB CID, not full weights
    // ... other fields
}
```

**Storage Flow:**
1. **Upload Model:** Node uploads weights to SVDB
2. **Get CID:** SVDB returns `artha://<cid>`
3. **On-Chain:** Store CID in `ModelRegistry`
4. **Retrieval:** Anyone can fetch from SVDB using CID

**Advantages:**
- ‚úÖ Scalable (unlimited size)
- ‚úÖ Verifiable (PoRep proofs)
- ‚úÖ Decentralized (multiple storage providers)
- ‚úÖ Cost-effective

**Alternative: IPFS/Filecoin**
- Similar to SVDB
- Can use as backup
- Bridge between SVDB and IPFS

**Recommendation:** **SVDB with IPFS/Filecoin as backup**

---

## üî• SECTION 5 ‚Äî Security & Anti-Cheating

### 14. Preventing Fake Gradient Submissions

#### **Cross-Verification**

**Implementation:**
- Assign same task to multiple nodes
- Compare gradient digests
- Require consensus (>50% match)

```solidity
function verifyGradients(bytes32 jobId, bytes32[] calldata nodePubkeys) external view returns (bool) {
    TrainProof[] memory proofs = trainProofs[jobId];
    uint256 matchCount = 0;
    
    for (uint i = 0; i < proofs.length; i++) {
        for (uint j = i + 1; j < proofs.length; j++) {
            if (proofs[i].gradientDigest == proofs[j].gradientDigest) {
                matchCount++;
            }
        }
    }
    
    // Require >50% consensus
    return matchCount > (proofs.length * proofs.length) / 4;
}
```

#### **Validator Committees**

**Committee Selection:**
- Randomly select 10-20 validator nodes
- Validators recompute gradients (spot check)
- Compare with submitted gradients
- Slash if mismatch

#### **Random Spot Checks**

**Implementation:**
- Randomly select 10% of jobs for full verification
- Validator nodes recompute from scratch
- Compare results
- Penalize cheaters

#### **Recommended: Hybrid Approach**
1. **Primary:** Cross-verification (redundancy)
2. **Secondary:** Random spot checks (10% of jobs)
3. **Tertiary:** Validator committees (for high-value jobs)

---

### 15. Slashing Malicious Nodes

**Contract:** `NodeCertRegistry.sol` + `ReputationRegistry.sol`

**Slashing Conditions:**
1. **Failed Verification:**
   - Gradient mismatch detected
   - Slash: 10% of stake

2. **Cheating Detection:**
   - Fake gradients submitted
   - Slash: 50% of stake

3. **Repeated Offenses:**
   - 3+ violations
   - Slash: 100% of stake + deactivation

**Implementation:**
```solidity
function slashNode(bytes32 nodePubkey, uint256 slashAmount, string calldata reason) external {
    NodeCert storage node = nodes[nodePubkey];
    require(node.stake >= slashAmount, "Insufficient stake");
    
    node.stake -= slashAmount;
    // Transfer to slashing pool or burn
    
    // Update reputation
    ReputationRegistry.subReputation(node.operator, slashAmount / 1e18);
    
    emit NodeSlashed(nodePubkey, slashAmount, reason);
}
```

---

## üî• SECTION 6 ‚Äî Result Merging & Final Model Publishing

### 16. Merging Updates into Global Model

#### **Aggregator Node**

**Architecture:**
- Dedicated aggregator nodes (high-reputation, high-bandwidth)
- Collect gradients from all nodes
- Compute FedAvg
- Broadcast updated model

**Implementation:**
```rust
// From services/ai-federation/src/main.rs
async fn trigger_aggregation(fed_id: String) -> Result<Vec<f64>> {
    let updates = gradient_updates.read().await;
    let job = fed_jobs.read().await.get(&fed_id)?;
    
    if let Some(grad_updates) = updates.get(&fed_id) {
        if grad_updates.len() >= job.participants.len().max(1) {
            // Aggregate using FedAvg
            let aggregated = if job.dp_enabled {
                secure_aggregate(grad_updates, 0.1)
            } else {
                federated_average(grad_updates)
            };
            
            return Ok(aggregated);
        }
    }
    
    Err("Not enough updates")
}
```

#### **Decentralized Committees**

**Committee-Based Aggregation:**
- Randomly select committee of 10-20 nodes
- Committee members aggregate gradients
- Consensus on aggregated model
- Broadcast to all nodes

#### **Weighted Averaging**

**FedAvg with Sample Weights:**
```rust
fn federated_average(updates: &[GradientUpdate]) -> Vec<f64> {
    let total_samples: u64 = updates.iter().map(|u| u.sample_count).sum();
    let num_params = updates[0].weights.len();
    let mut aggregated = vec![0.0; num_params];
    
    // Weighted average: sum(w_i * n_i) / sum(n_i)
    for update in updates {
        let weight = update.sample_count as f64 / total_samples as f64;
        for (i, w) in update.weights.iter().enumerate() {
            aggregated[i] += w * weight;
        }
    }
    
    aggregated
}
```

**Recommended:** **Aggregator Node with Decentralized Committees as Backup**

---

### 17. Final Global Model Storage

#### **IPFS (Primary)**

**Storage:**
- Upload final model to IPFS
- Get CID
- Store CID on-chain

#### **Arweave (Permanent)**

**Storage:**
- Upload to Arweave for permanent storage
- One-time payment
- Immutable

#### **SVDB (Recommended)**

**Storage:**
- Upload to SVDB (ArthaChain's storage)
- Get `artha://<cid>`
- Store CID in `ModelRegistry`
- Verifiable via PoRep

**Implementation:**
```solidity
// ModelRegistry.sol
function publishModel(bytes32 modelId, bytes32 checkpointCid) external {
    Model storage model = models[modelId];
    require(model.owner == msg.sender, "Not owner");
    
    // Store final model CID
    model.checkpoints.push(checkpointCid);
    
    emit ModelPublished(modelId, checkpointCid);
}
```

**Recommendation:** **SVDB (Primary) + IPFS (Backup) + Arweave (Permanent)**

---

### 18. User Access to Final AI Model

#### **API Access**

**REST API:**
```
POST /ai/infer
{
    "modelCid": "artha://...",
    "input": "...",
    "maxTokens": 100
}
```

#### **RPC Access**

**gRPC/JSON-RPC:**
- Direct RPC calls to inference nodes
- Lower latency
- Better for real-time

#### **Embedded Model**

**On-Device:**
- Download model to device
- Run inference locally
- No network required

**Recommendation:** **API (Primary) + RPC (Advanced) + Embedded (Offline)**

---

## üî• SECTION 7 ‚Äî Full Pipeline Flow

### 19. End-to-End Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FULL PIPELINE FLOW                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. NODE JOINS
   ‚îÇ
   ‚îú‚îÄ> Node registers on NodeCertRegistry.sol
   ‚îú‚îÄ> Deposits stake (1-100 ARTH)
   ‚îú‚îÄ> Submits hardware capabilities (GPU, RAM, bandwidth)
   ‚îú‚îÄ> Gets nodePubkey (bytes32)
   ‚îî‚îÄ> Starts heartbeat (every hour)

2. RECEIVES TRAINING TASK
   ‚îÇ
   ‚îú‚îÄ> AIJobManager.sol: submitTrain() called
   ‚îú‚îÄ> Job assigned to node (via scheduler)
   ‚îú‚îÄ> Node receives:
   ‚îÇ   ‚îú‚îÄ modelCid (artha://...)
   ‚îÇ   ‚îú‚îÄ datasetCid (artha://...)
   ‚îÇ   ‚îú‚îÄ training params (epochs, batch size, etc.)
   ‚îÇ   ‚îî‚îÄ jobId (bytes32)
   ‚îî‚îÄ> Node downloads model + dataset from SVDB

3. TRAINS LOCALLY
   ‚îÇ
   ‚îú‚îÄ> Node loads model weights
   ‚îú‚îÄ> Node loads training data (encrypted, decrypted locally)
   ‚îú‚îÄ> Forward pass (off-chain)
   ‚îú‚îÄ> Backward pass (off-chain)
   ‚îú‚îÄ> Gradient computation (off-chain)
   ‚îú‚îÄ> Every N steps:
   ‚îÇ   ‚îú‚îÄ Compute gradient digest (keccak256)
   ‚îÇ   ‚îú‚îÄ Compute loss digest
   ‚îÇ   ‚îú‚îÄ Compute weights digest
   ‚îÇ   ‚îî‚îÄ Sign with node private key
   ‚îî‚îÄ> Store checkpoints in SVDB

4. UPLOADS GRADIENTS
   ‚îÇ
   ‚îú‚îÄ> Node uploads gradients to SVDB
   ‚îú‚îÄ> Gets gradientCid (artha://...)
   ‚îú‚îÄ> Computes gradient digest
   ‚îî‚îÄ> Prepares proof

5. COMMITS HASH ON-CHAIN
   ‚îÇ
   ‚îú‚îÄ> ProofOfCompute.sol: recordTrainProof()
   ‚îú‚îÄ> Submits:
   ‚îÇ   ‚îú‚îÄ jobId
   ‚îÇ   ‚îú‚îÄ step
   ‚îÇ   ‚îú‚îÄ lossDigest
   ‚îÇ   ‚îú‚îÄ gradientDigest
   ‚îÇ   ‚îú‚îÄ weightsDigest
   ‚îÇ   ‚îú‚îÄ nodePubkey
   ‚îÇ   ‚îî‚îÄ signature
   ‚îî‚îÄ> Emits TrainProofRecorded event

6. GETS VERIFIED
   ‚îÇ
   ‚îú‚îÄ> Validator nodes check proof
   ‚îú‚îÄ> Cross-verification (compare with other nodes)
   ‚îú‚îÄ> Random spot checks (10% of jobs)
   ‚îú‚îÄ> Gradient sampling (if challenged)
   ‚îî‚îÄ> Verification result recorded

7. RECEIVES REWARD
   ‚îÇ
   ‚îú‚îÄ> ProofOfCompute.sol: finalize()
   ‚îú‚îÄ> Calculates reward:
   ‚îÇ   ‚îú‚îÄ baseReward = gpuSeconds * 0.001 ARTH
   ‚îÇ   ‚îú‚îÄ accuracyBonus (if applicable)
   ‚îÇ   ‚îî‚îÄ qualityMultiplier
   ‚îú‚îÄ> Transfers ARTH to node operator
   ‚îî‚îÄ> Updates reputation in ReputationRegistry

8. MODEL UPDATES PUBLISHED
   ‚îÇ
   ‚îú‚îÄ> Aggregator collects gradients from all nodes
   ‚îú‚îÄ> Computes FedAvg (federated average)
   ‚îú‚îÄ> Uploads aggregated model to SVDB
   ‚îú‚îÄ> Gets new modelCid
   ‚îú‚îÄ> ModelRegistry.sol: addCheckpoint()
   ‚îú‚îÄ> Stores checkpointCid on-chain
   ‚îî‚îÄ> Broadcasts new model to all nodes

9. NEXT ROUND (if federated learning)
   ‚îÇ
   ‚îú‚îÄ> Nodes download updated model
   ‚îú‚îÄ> Continue training from updated weights
   ‚îî‚îÄ> Repeat steps 3-8

10. FINAL MODEL PUBLISHED
    ‚îÇ
    ‚îú‚îÄ> After all rounds complete
    ‚îú‚îÄ> Final model uploaded to SVDB
    ‚îú‚îÄ> ModelRegistry.sol: publishModel()
    ‚îú‚îÄ> Model available via API/RPC
    ‚îî‚îÄ> Users can access via Diigoo interface
```

---

## üî• SVDB & DID Integration

### Why SVDB Instead of Other Decentralized Databases?

#### **SVDB Advantages:**

1. **Verifiable Storage:**
   - PoRep (Proof of Replication) proofs
   - Continuous verification
   - Auto-slashing for failures

2. **Content Addressing:**
   - CID-based (similar to IPFS)
   - Deterministic (same content = same CID)
   - Immutable

3. **Built-in Marketplace:**
   - Storage provider marketplace
   - Dynamic pricing
   - SLA enforcement

4. **Privacy & Access Control:**
   - Client-side encryption
   - DID-based access control
   - TEE support

5. **Native Integration:**
   - Built into ArthaChain
   - On-chain contracts (SVDBPoRep, OfferBook)
   - Seamless integration with AI training

#### **Comparison:**

| Feature | SVDB | IPFS | Filecoin | Arweave |
|---------|------|------|----------|---------|
| Verifiable Proofs | ‚úÖ PoRep | ‚ùå | ‚úÖ PoRep | ‚úÖ PoA |
| Marketplace | ‚úÖ Built-in | ‚ùå | ‚úÖ | ‚ùå |
| DID Integration | ‚úÖ Native | ‚ùå | ‚ùå | ‚ùå |
| Cost | Low | Free | Medium | One-time |
| Permanent | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |

**Recommendation:** **SVDB (Primary) + IPFS (Backup) + Arweave (Permanent)**

---

### How DID Assures User Security

#### **DID (Decentralized Identity) System:**

**Contract:** `ArthaDIDRegistry.sol`

**Structure:**
```solidity
struct DIDDocument {
    bytes32 didHash;      // keccak256(pubkey)
    address owner;        // controller
    bytes32 authKey;      // Ed25519 (authentication)
    bytes32 encKey;       // X25519 (encryption)
    bytes32 metaCid;      // SVDB CID of full DID Doc
    uint64 createdAt;
    uint64 updatedAt;
    bool revoked;
}
```

#### **Security Features:**

1. **Cryptographic Identity:**
   - DID = `did:artha:<hash>`
   - Hash of public key (immutable)
   - Cannot be forged

2. **Key Rotation:**
   - Can rotate keys without changing DID
   - Old keys invalidated
   - New keys activated

3. **Access Control:**
   - DID-based permissions
   - Fine-grained access policies
   - Revocable

4. **Privacy:**
   - DID doesn't reveal personal info
   - Pseudonymous
   - Can link to real identity (optional)

#### **User Security Assurance:**

1. **Data Ownership:**
   - User data encrypted with user's DID
   - Only user (or authorized parties) can decrypt
   - User controls access

2. **Consent Management:**
   - User grants consent via DID signature
   - Consent recorded on-chain
   - Revocable at any time

3. **Audit Trail:**
   - All access logged with DID
   - Immutable record
   - User can audit who accessed their data

4. **Data Portability:**
   - User can export data (encrypted)
   - Can move to other systems
   - DID follows user

---

### How AI Model Developers Get Data Without User Details

#### **Privacy-Preserving Data Access:**

**Problem:** Developers need training data, but users want privacy.

**Solution: Differential Privacy + Federated Learning + DID**

#### **Mechanism 1: Federated Learning**

**How It Works:**
- Data stays on user's device
- Only gradients are shared (not raw data)
- Developer gets aggregated gradients
- Never sees individual user data

**Implementation:**
```rust
// User's device trains locally
let local_gradients = train_on_local_data(user_data);

// Only gradients shared (not data)
submit_gradient(fed_id, local_gradients);

// Aggregator computes FedAvg
let aggregated = federated_average(all_gradients);

// Developer gets aggregated model (no user data)
```

#### **Mechanism 2: Differential Privacy**

**How It Works:**
- Add noise to gradients before sharing
- Protects individual user data
- Developer gets noisy gradients
- Model still learns (noise averages out)

**Implementation:**
```rust
fn secure_aggregate(updates: &[GradientUpdate], dp_scale: f64) -> Vec<f64> {
    let mut aggregated = federated_average(updates);
    
    // Add Laplacian noise for differential privacy
    use rand::Rng;
    let mut rng = rand::thread_rng();
    for weight in &mut aggregated {
        let noise = rng.gen::<f64>() * dp_scale;
        *weight += noise;
    }
    
    aggregated
}
```

#### **Mechanism 3: DID-Based Access Control**

**How It Works:**
- User data encrypted with user's DID
- Developer requests access via DID
- User grants permission (signed with DID)
- Developer gets encrypted data
- Developer decrypts with permission key

**Implementation:**
```solidity
// User grants access
function grantDataAccess(
    bytes32 datasetCid,
    bytes32 developerDid,
    bytes32 permissionKey
) external {
    // User signs with their DID
    // Permission stored on-chain
    // Developer can access encrypted data
}
```

#### **Mechanism 4: Anonymized Datasets**

**How It Works:**
- User data anonymized before sharing
- PII removed (names, addresses, etc.)
- Only relevant features kept
- Developer gets anonymized dataset

**Implementation:**
```rust
// Anonymize user data
let anonymized = anonymize_data(user_data, {
    remove_pii: true,
    k_anonymity: 5,  // At least 5 users per group
    differential_privacy: true,
});

// Upload anonymized data to SVDB
let cid = upload_to_svdb(anonymized);

// Developer accesses anonymized dataset
let dataset = download_from_svdb(cid);
```

#### **Complete Flow:**

```
1. USER UPLOADS DATA
   ‚îÇ
   ‚îú‚îÄ> Data encrypted with user's DID
   ‚îú‚îÄ> Uploaded to SVDB (encrypted)
   ‚îú‚îÄ> User sets access policy (DID-based)
   ‚îî‚îÄ> Data CID stored on-chain

2. DEVELOPER REQUESTS ACCESS
   ‚îÇ
   ‚îú‚îÄ> Developer (with DID) requests access
   ‚îú‚îÄ> User grants permission (signed with DID)
   ‚îú‚îÄ> Permission recorded on-chain
   ‚îî‚îÄ> Developer gets decryption key

3. DEVELOPER ACCESSES DATA
   ‚îÇ
   ‚îú‚îÄ> Developer downloads encrypted data from SVDB
   ‚îú‚îÄ> Decrypts with permission key
   ‚îú‚îÄ> Gets anonymized/differentially private data
   ‚îî‚îÄ> Never sees user's personal details

4. TRAINING (Federated Learning)
   ‚îÇ
   ‚îú‚îÄ> Developer trains model using federated learning
   ‚îú‚îÄ> Users train locally (data stays on device)
   ‚îú‚îÄ> Only gradients shared (not raw data)
   ‚îú‚îÄ> Differential privacy noise added
   ‚îî‚îÄ> Developer gets aggregated model (no user data)
```

#### **Privacy Guarantees:**

1. **User Data Never Leaves Device (Federated Learning):**
   - Data stays encrypted on user's device
   - Only gradients shared
   - Developer never sees raw data

2. **Differential Privacy:**
   - Individual contributions protected
   - Noise prevents inference attacks
   - Mathematical privacy guarantee

3. **DID-Based Access Control:**
   - User controls who accesses data
   - Revocable permissions
   - Audit trail

4. **Anonymization:**
   - PII removed before sharing
   - k-anonymity protection
   - Developer gets anonymized data only

**Result:** Developers get training data without seeing user details!

---

## Summary

### Key Recommendations:

1. **Smart Contracts:**
   - Use `NodeCertRegistry` for node management
   - Use `ProofOfCompute` for training proofs
   - Use `ReputationRegistry` for reputation

2. **Training Strategy:**
   - Federated Learning (data parallelism)
   - 1.3B parameter model (sweet spot)
   - SSM architecture (CPU-friendly)

3. **Verification:**
   - Redundancy (primary)
   - Random spot checks (secondary)
   - Gradient sampling (tertiary)

4. **Storage:**
   - SVDB for model weights
   - On-chain only CIDs (not full data)

5. **Privacy:**
   - Federated Learning (data stays on device)
   - Differential Privacy (noise added)
   - DID-based access control

6. **Bandwidth:**
   - Minimum 20 Mbps per node
   - Gradient compression recommended
   - 1 round/hour frequency

---

**üåê Built for a Sovereign, Privacy-Preserving AI Future**

